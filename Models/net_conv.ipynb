{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f879a7-82cc-4713-a59e-1e35aafce9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "# TCN\n",
    "class tcn(nn.Module):\n",
    "    def __init__(self, tcn_size):\n",
    "        super(tcn, self).__init__()\n",
    "        self.tcn_size = tcn_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_new = x[:, :, :-self.tcn_size]\n",
    "        return x_new.contiguous()\n",
    "\n",
    "\n",
    "# Deformable\n",
    "class DeformableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, dilation, pad_mode):\n",
    "        super(DeformableConv2d, self).__init__()\n",
    "\n",
    "        self.padding = (padding, 0)\n",
    "        self.dilation = (dilation, 1)\n",
    "        self.ks = (kernel_size, 1)\n",
    "\n",
    "        self.offset_conv = nn.Conv2d(in_channels, 2 * kernel_size, self.ks, padding=self.padding, dilation=self.dilation, padding_mode=pad_mode, bias=True)\n",
    "        nn.init.constant_(self.offset_conv.weight, 0.)\n",
    "        nn.init.constant_(self.offset_conv.bias, 0.)\n",
    "\n",
    "        self.modulator_conv = nn.Conv2d(in_channels, kernel_size, self.ks, padding=self.padding, dilation=self.dilation, padding_mode=pad_mode, bias=True)\n",
    "        nn.init.constant_(self.modulator_conv.weight, 0.)\n",
    "        nn.init.constant_(self.modulator_conv.bias, 0.)\n",
    "\n",
    "        self.regular_conv = nn.Conv2d(in_channels, out_channels, self.ks, padding=self.padding, dilation=self.dilation, padding_mode=pad_mode, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        max_offset = max(h, w) / 4.\n",
    "        offset = self.offset_conv(x).clamp(-max_offset, max_offset)\n",
    "        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n",
    "\n",
    "        x = torchvision.ops.deform_conv2d(input=x, offset=offset, weight=self.regular_conv.weight, bias=self.regular_conv.bias, padding=self.padding, dilation=self.dilation, mask=modulator)\n",
    "        return x\n",
    "\n",
    "\n",
    "# One Conv. block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, model, c_in, c_out, ks, pad, dil, deformable):\n",
    "        super(Block, self).__init__()\n",
    "        self.model = model\n",
    "        self.deform = deformable\n",
    "\n",
    "        if model == 'CDIL':\n",
    "            pad_mode = 'circular'\n",
    "        else:\n",
    "            pad_mode = 'zeros'\n",
    "\n",
    "        if self.deform:\n",
    "            self.conv = DeformableConv2d(c_in, c_out, ks, pad, dil, pad_mode)\n",
    "        else:\n",
    "            self.conv = weight_norm(nn.Conv1d(c_in, c_out, ks, padding=pad, dilation=dil, padding_mode=pad_mode))\n",
    "            self.conv.weight.data.normal_(0, 0.01)\n",
    "            self.conv.bias.data.normal_(0, 0.01)\n",
    "\n",
    "        if model == 'TCN':\n",
    "            self.cut = tcn(pad)\n",
    "            self.tcn = nn.Sequential(self.conv, self.cut)\n",
    "\n",
    "        self.res = nn.Conv1d(c_in, c_out, kernel_size=(1,)) if c_in != c_out else None\n",
    "        if self.res is not None:\n",
    "            self.res.weight.data.normal_(0, 0.01)\n",
    "            self.res.bias.data.normal_(0, 0.01)\n",
    "\n",
    "        self.nonlinear = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model == 'TCN':\n",
    "            net = self.tcn\n",
    "        else:\n",
    "            net = self.conv\n",
    "\n",
    "        if self.deform:\n",
    "            x_2d = x.unsqueeze(-1)\n",
    "            out = net(x_2d)\n",
    "            res = x if self.res is None else self.res(x)\n",
    "            y = self.nonlinear(out) + res.unsqueeze(-1)\n",
    "            return y.squeeze(-1)\n",
    "        else:\n",
    "            out = net(x)\n",
    "            res = x if self.res is None else self.res(x)\n",
    "            return self.nonlinear(out) + res\n",
    "\n",
    "\n",
    "# Conv. blocks\n",
    "class ConvPart(nn.Module):\n",
    "    def __init__(self, model, dim_in, hidden_channels, ks, deformable, dynamic):\n",
    "        super(ConvPart, self).__init__()\n",
    "        layers = []\n",
    "        num_layer = len(hidden_channels)\n",
    "        begin = 1 if dynamic else 0\n",
    "        for i in range(begin, num_layer):\n",
    "            this_in = dim_in if i == 0 else hidden_channels[i - 1]\n",
    "            this_out = hidden_channels[i]\n",
    "            if model == 'CNN':\n",
    "                this_dilation = 1\n",
    "                this_padding = int((ks - 1) / 2)\n",
    "            else:\n",
    "                this_dilation = 2 ** i\n",
    "                if model == 'TCN':\n",
    "                    this_padding = this_dilation * (ks - 1)\n",
    "                elif model == 'CDIL' or model == 'DIL':\n",
    "                    this_padding = int(this_dilation*(ks-1)/2)\n",
    "                else:\n",
    "                    print('no this model.')\n",
    "                    sys.exit()\n",
    "            if i < (num_layer-3):\n",
    "                layers += [Block(model, this_in, this_out, ks, this_padding, this_dilation, False)]\n",
    "            else:\n",
    "                layers += [Block(model, this_in, this_out, ks, this_padding, this_dilation, deformable)]\n",
    "        self.conv_net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_net(x)\n",
    "\n",
    "\n",
    "# Conv. + classifier\n",
    "class CONV(nn.Module):\n",
    "    def __init__(self, model, input_size, output_size, num_channels, kernel_size, deformable=False, dynamic=False, use_embed=False, char_vocab=None, fix_length=True):\n",
    "        super(CONV, self).__init__()\n",
    "        self.model = model\n",
    "        self.dynamic = dynamic\n",
    "        self.use_embed = use_embed\n",
    "        self.fix_lengh = fix_length\n",
    "\n",
    "        if self.use_embed:\n",
    "            self.embedding = nn.Embedding(char_vocab, input_size)\n",
    "\n",
    "        self.conv = ConvPart(model, input_size, num_channels, kernel_size, deformable, dynamic)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        print(\"Input Shape:\", x.shape)\n",
    "    \n",
    "        if self.use_embed:\n",
    "            x = self.embedding(x)\n",
    "            print(\"After Embedding Shape:\", x.shape)\n",
    "    \n",
    "        if not self.dynamic:\n",
    "            x = x.permute(0, 2, 1).to(dtype=torch.float)\n",
    "            print(\"After Permute Shape:\", x.shape)\n",
    "        y_conv = self.conv(x)\n",
    "\n",
    "        if self.model == 'TCN':\n",
    "            if self.fix_lengh:\n",
    "                y_class = y_conv[:, :, -1]\n",
    "            else:\n",
    "                P = mask.unsqueeze(1).expand(y_conv.size(0), y_conv.size(1)).unsqueeze(2)\n",
    "                y_class = y_conv.gather(2, P).squeeze(2)\n",
    "        else:\n",
    "            y_class = torch.mean(y_conv, dim=2)\n",
    "\n",
    "        y = self.linear(y_class)\n",
    "        return y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
